{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## Potential Talents - Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "# Job Title Similarity using LLMs-as-Rankers\n",
    "\n",
    "### Objectives\n",
    "Retrieve the most similar job titles to a query **using LLMs as deterministic rankers** and compare their rankings against the **existing embeddings + cosine** baseline.\n",
    "\n",
    "### Constraints\n",
    "- Local execution on **GTX 1080 Ti**.\n",
    "- **Deterministic** decoding (no sampling): `temperature=0`.\n",
    "- Score **only** from the provided list of 105 titles; **no generation** of new titles.\n",
    "- LLM outputs are **numeric similarity scores (0â€“100)** in strict JSON, then ranked.\n",
    "\n",
    "### Models (initial)\n",
    "- **FLAN-T5-Large** (ðŸ¤—, encoderâ€“decoder).\n",
    "- **Phi-3-mini-4k-instruct** (ðŸ¤—).  \n",
    "\n",
    "### Method Overview\n",
    "- **Baseline (done already at part 3):** embeddings + cosine similarity â†’ per-query rankings and scores.\n",
    "- **LLM-as-Ranker:** for each `(query, title)` ask the model for an integer score **0â€“100**\n",
    "  Batch candidates to keep context small; parse JSON; rank by score.\n",
    "  \n",
    "### Display & Evaluation\n",
    "- **Notebook display:** same format as before  \n",
    "  Query: <query>\n",
    "   0.793 <title_raw>\n",
    "   0.748 <title_raw>\n",
    "\n",
    "- **Files:** per-model/per-query CSV with `id, score_llm, rank_llm, title_raw, title_clean`.\n",
    "- **Comparison metric:** **nDCG@k** (reuse function from the embedding notebook, part 3).  \n",
    "\n",
    "---\n",
    "\n",
    "## Summary / Roadmap\n",
    "\n",
    "0) Setup & Data  \n",
    "1) Load **baseline results** (embeddings + cosine) for each query  \n",
    "2) Load **LLM model** (start with FLAN-T5-Large)  \n",
    "3) **Prompt & batch scoring** â†’ JSON `{id, score}`  \n",
    "4) Build **LLM ranking** and **print** in the prior format; save CSV  \n",
    "5) **Compare** to baseline via **nDCG@k**  \n",
    "6) Repeat Steps 2â€“5 for **Phi-3-mini**  \n",
    "7) Create a **short summary table** (per model Ã— query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "### Step 0 - Imports, config, folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os, json, math, re, random, time, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# HF\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 23\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# paths\n",
    "DATA_DIR = \"data\"\n",
    "OUT_DIR  = \"outputs\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "QUERIES = [\"data scientist\", \"machine learning engineer\", \"backend developer\", \"product manager\"]  # same queries from Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d545eb",
   "metadata": {},
   "source": [
    "### Step 1 - Load titles and make a clean field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d845f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"potential_talents.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b98037",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df[\"job_title\"].astype(str).tolist()\n",
    "len(titles), titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c7ee5",
   "metadata": {},
   "source": [
    "### Step 2 - Load SBERT top-10 baseline (as-is, from the previous project part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb96aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your SBERT baseline as produced in Part 3 (no changes to schema)\n",
    "BASELINE_TOP10_CSV = os.path.join(OUT_DIR, \"sbert_ranking_output.csv\")\n",
    "base = pd.read_csv(BASELINE_TOP10_CSV)\n",
    "\n",
    "print(base.head(3))\n",
    "print(\"Queries in baseline:\", base[\"query\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546e899",
   "metadata": {},
   "source": [
    "### Step 3 - Pretty printer (same style as Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ranking(query, rows_df, score_col=\"score\", title_col=\"job_titles\", top_k=10):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for _, r in rows_df.head(top_k).iterrows():\n",
    "        print(f\"   {r[score_col]: .3f}  {r[title_col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8800280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in QUERIES:\n",
    "    print_ranking(query, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db80fed",
   "metadata": {},
   "source": [
    "### Step 4 - Load the first LLM (FLAN-T5-Large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"built with CUDA:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef92698",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/flan-t5-large\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mdl.to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=mdl,\n",
    "    tokenizer=tok,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac1b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f9861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pot_Tals_LLM_Env",
   "language": "python",
   "name": "pot-tals_llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
