{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## Potential Talents - Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "# Job Title Similarity using LLMs-as-Rankers\n",
    "\n",
    "### Objective\n",
    "Given a query, ask a small LLM to score **all 104 job titles at once** (0–100, one score per line, same order), then rank the scores to compare the **top-10** with other results (from embeddings + cosine or other LLMs results)\n",
    "\n",
    "### Constraints\n",
    "- Local GPU: **GTX 1080 Ti**.\n",
    "- **Deterministic** generation: `do_sample=False`, `num_beams=1`.\n",
    "\n",
    "### Models (initial)\n",
    "- **1:** `microsoft/phi-3-mini-4k-instruct` (4k context, small & GPU-friendly).\n",
    "- **2:** `google/gemma-2-2b-it` (8k context, very small).\n",
    "- (After some tests we will avoid FLAN-T5 here due to the ~512 token input limit.)\n",
    "\n",
    "### Method\n",
    "1) Load SBERT top-10 baseline (from Part 3).  \n",
    "2) Load a small **causal LM**.  \n",
    "3) Build a prompt that lists all **104** titles (numbered).  \n",
    "4) Generate **104 lines of integers**; parse → rank; print top-10; save top-10 CSV (`query,score,job_titles`).  \n",
    "5) Repeat for the 4 queries; later compute nDCG@10 and compare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "### Step 0 - Imports, config, folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ce1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os, json, math, re, random, time, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# HF\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2188f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 23\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# paths\n",
    "DATA_DIR = \"data\"\n",
    "OUT_DIR  = \"outputs\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "QUERIES = [\"data scientist\", \"machine learning engineer\", \"backend developer\", \"product manager\"]  # same queries from Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d545eb",
   "metadata": {},
   "source": [
    "### Step 1 - Load titles and make a clean field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d845f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"potential_talents.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b98037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,\n",
       " ['2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional',\n",
       "  'Native English Teacher at EPIK (English Program in Korea)',\n",
       "  'Aspiring Human Resources Professional',\n",
       "  'People Development Coordinator at Ryan',\n",
       "  'Advisory Board Member at Celal Bayar University'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = df[\"job_title\"].astype(str).tolist()\n",
    "len(titles), titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c7ee5",
   "metadata": {},
   "source": [
    "### Step 2 - Load SBERT top-10 baseline (as-is, from the previous project part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb96aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            query     score                                         job_titles\n",
      "0  data scientist  0.595830  Information Systems Specialist and Programmer ...\n",
      "1  data scientist  0.494619                       Human Resources Professional\n",
      "2  data scientist  0.456588           Junior MES Engineer| Information Systems\n",
      "Queries in baseline: ['data scientist' 'machine learning engineer' 'backend developer'\n",
      " 'product manager']\n"
     ]
    }
   ],
   "source": [
    "# Load your SBERT baseline as produced in Part 3 (no changes to schema)\n",
    "BASELINE_TOP10_CSV = os.path.join(OUT_DIR, \"sbert_ranking_output.csv\")\n",
    "base = pd.read_csv(BASELINE_TOP10_CSV)\n",
    "\n",
    "print(base.head(3))\n",
    "print(\"Queries in baseline:\", base[\"query\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546e899",
   "metadata": {},
   "source": [
    "### Step 3 - Pretty printer (same style as Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2643cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ranking(query, rows_df, score_col=\"score\", title_col=\"job_titles\", top_k=10):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for _, r in rows_df.head(top_k).iterrows():\n",
    "        print(f\"   {r[score_col]: .3f}  {r[title_col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8800280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: data scientist\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n",
      "\n",
      "Query: backend developer\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n",
      "\n",
      "Query: product manager\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n"
     ]
    }
   ],
   "source": [
    "for query in QUERIES:\n",
    "    print_ranking(query, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db80fed",
   "metadata": {},
   "source": [
    "### Step 4 - Load a small LLM (Phi-3-mini from Microsoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5c1958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124\n",
      "built with CUDA: 12.4\n",
      "cuda available: True\n",
      "gpu: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"built with CUDA:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef92698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef6b969936f45548b24f8175a5d1bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "394583c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a proper chat prompt\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a calculator. Reply with digits only.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"Return the number 7.\"}\n",
    "]\n",
    "prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca4e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode & generate (greedy)\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "eos = [tok.eos_token_id]\n",
    "try:\n",
    "    eos.append(tok.convert_tokens_to_ids(\"<|end|>\"))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e8d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = mdl.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=3,\n",
    "    eos_token_id=eos,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e410f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "out = tok.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "print(out)  # -> 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f9861",
   "metadata": {},
   "source": [
    "### Step 5 — turn LLM into a job title ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8837f",
   "metadata": {},
   "source": [
    "We will turn the LLM into a ranker by asking it to assign an integer score (0–100) to each raw job title for a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07e1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_all_chat(query: str, titles: list[str]) -> str:\n",
    "    lines = \"\\n\".join(f\"{i+1}) {t}\" for i, t in enumerate(titles))\n",
    "    rubric = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Rate each candidate with an integer 0–100 using the FULL scale:\\n\"\n",
    "        \" • 90–100 = exact/near-exact role match\\n\"\n",
    "        \" • 70–89  = same discipline or very similar role\\n\"\n",
    "        \" • 40–69  = related/adjacent\\n\"\n",
    "        \" • 10–39  = mostly unrelated\\n\"\n",
    "        \" • 0–9    = completely unrelated\\n\"\n",
    "        \"Use diverse scores; do NOT give 0 or 100 to many candidates.\\n\"\n",
    "        \"Ignore employer names, locations, programs.\\n\"\n",
    "        \"Output EXACTLY one integer per line, in the SAME ORDER as the candidates. No words, no punctuation.\"\n",
    "    )\n",
    "    # Non-extreme example\n",
    "    example = \"Example for 3 candidates:\\n82\\n41\\n7\"\n",
    "    user = f'Query: \"{query}\"\\n\\nCandidates:\\n{lines}\\n\\n{example}'\n",
    "    msgs = [{\"role\": \"system\", \"content\": rubric},\n",
    "            {\"role\": \"user\",   \"content\": user}]\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd5fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scores_n(out: str, n: int) -> list[int]:\n",
    "    # prefer last int per non-empty line; fallback to last N ints in whole text\n",
    "    lines = [l.strip() for l in out.splitlines() if l.strip()]\n",
    "    scores = []\n",
    "    \n",
    "    for line in lines:\n",
    "        ints = re.findall(r\"-?\\d+\", line)\n",
    "        if ints:\n",
    "            scores.append(int(ints[-1]))\n",
    "        if len(scores) >= n:\n",
    "            break\n",
    "        \n",
    "    if len(scores) < n:\n",
    "        all_ints = [int(x) for x in re.findall(r\"-?\\d+\", out)]\n",
    "        scores = all_ints[-n:]\n",
    "        \n",
    "    scores = [max(0, min(100, int(s))) for s in scores]\n",
    "    \n",
    "    # if still short, add a padding\n",
    "    if len(scores) < n:  \n",
    "        scores += [0] * (n - len(scores))\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b170f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_all_titles_once(query: str, titles: list[str], max_new_tokens: int = 300, build_fn=build_prompt_all_chat):\n",
    "    prompt = build_fn(query, titles)\n",
    "    print(\"Prompt tokens:\", len(tok(prompt)[\"input_ids\"]))\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "    gen = mdl.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=[tok.eos_token_id],\n",
    "        # optional: ensure we don’t stop too early\n",
    "        min_new_tokens=min(104, max_new_tokens-1),\n",
    "    )\n",
    "    out_text = tok.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    scores = parse_scores_n(out_text, len(titles))\n",
    "    df = pd.DataFrame({\"idx\": range(len(titles)), \"score\": scores})\n",
    "    df[\"job_titles\"] = df[\"idx\"].map(lambda i: titles[i])\n",
    "    df = df.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return df, out_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ee9d3",
   "metadata": {},
   "source": [
    "Test **build_prompt_all_chat**, **parse_scores_n** and **score_all_tittles_once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1c94d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEMO PROMPT (first 30 lines) ===\n",
      "<|system|>\n",
      "You are a recruiter scoring job-title similarity to the query.\n",
      "Rate each candidate with an integer 0–100 using the FULL scale:\n",
      " • 90–100 = exact/near-exact role match\n",
      " • 70–89  = same discipline or very similar role\n",
      " • 40–69  = related/adjacent\n",
      " • 10–39  = mostly unrelated\n",
      " • 0–9    = completely unrelated\n",
      "Use diverse scores; do NOT give 0 or 100 to many candidates.\n",
      "Ignore employer names, locations, programs.\n",
      "Output EXACTLY one integer per line, in the SAME ORDER as the candidates. No words, no punctuation.<|end|>\n",
      "<|user|>\n",
      "Query: \"data scientist\"\n",
      "\n",
      "Candidates:\n",
      "1) 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "2) Native English Teacher at EPIK (English Program in Korea)\n",
      "3) Aspiring Human Resources Professional\n",
      "4) People Development Coordinator at Ryan\n",
      "5) Advisory Board Member at Celal Bayar University\n",
      "\n",
      "Example for 3 candidates:\n",
      "82\n",
      "41\n",
      "7<|end|>\n",
      "<|assistant|>\n",
      "Token count (subset): 279\n"
     ]
    }
   ],
   "source": [
    "test_query = \"data scientist\"\n",
    "\n",
    "# A Tiny subset to inspect everything\n",
    "subset = titles[:5]\n",
    "\n",
    "demo_prompt = build_prompt_all_chat(test_query, subset)\n",
    "print(\"=== DEMO PROMPT (first 30 lines) ===\")\n",
    "print(\"\\n\".join(demo_prompt.splitlines()[:30]))\n",
    "print(\"Token count (subset):\", len(tok(demo_prompt)[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0dd40f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 279\n",
      "\n",
      "=== RAW MODEL OUTPUT (subset) ===\n",
      "0\n",
      "0\n",
      "7\n",
      "41\n",
      "0 Query: \"software engineer specializing in machine learning\"\n",
      "\n",
      "Candidates:\n",
      "1) 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and asp\n"
     ]
    }
   ],
   "source": [
    "df_sub, raw_sub = score_all_titles_once(test_query, subset, max_new_tokens=60)\n",
    "print(\"\\n=== RAW MODEL OUTPUT (subset) ===\")\n",
    "print(raw_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5738ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed scores (subset): [0, 0, 7, 41, 0]\n",
      "\n",
      "Paired (score, title) in ranked order:\n",
      " 41  People Development Coordinator at Ryan\n",
      "  7  Aspiring Human Resources Professional\n",
      "  0  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "  0  Native English Teacher at EPIK (English Program in Korea)\n",
      "  0  Advisory Board Member at Celal Bayar University\n"
     ]
    }
   ],
   "source": [
    "scores_sub = parse_scores_n(raw_sub, len(subset))\n",
    "print(\"\\nParsed scores (subset):\", scores_sub)\n",
    "print(\"\\nPaired (score, title) in ranked order:\")\n",
    "# the output DataFrame from `socre_all_titles_once`, df_sub, is sorted in not ascending order\n",
    "for _, r in df_sub.iterrows():\n",
    "    print(f\"{r['score']:>3}  {r['job_titles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bafef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token count (full): 1928\n"
     ]
    }
   ],
   "source": [
    "# B) One full run (preview only; avoids flooding output)\n",
    "full_prompt = build_prompt_all_chat(test_query, titles)\n",
    "print(\"\\nToken count (full):\", len(tok(full_prompt)[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2197f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation of long strings\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d261d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 1928\n"
     ]
    }
   ],
   "source": [
    "df_full, raw_full = score_all_titles_once(test_query, titles, max_new_tokens=300)\n",
    "print(\"\\nFull run: got\", len(df_full), \"scores.\")\n",
    "print(\"Top-3 preview:\")\n",
    "print(df_full.head(10)[[\"score\", \"job_titles\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ranking(query, rows_df, top_k=10):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for _, r in rows_df.head(top_k).iterrows():\n",
    "        print(f\"   {r['score']/100: .3f}  {r['job_titles']}\")\n",
    "\n",
    "def run_query_full(queries: list[str], model_tag: str = \"phi3_mini_4k\"):\n",
    "    \n",
    "    out_dir = os.path.join(OUT_DIR, \"llm\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    top10_blocks = []\n",
    "    \n",
    "    for query in queries:\n",
    "        df_rank, raw = score_all_titles_once(query, titles)\n",
    "        print_ranking(query, df_rank, top_k=10)\n",
    "\n",
    "        df_q = df_rank.head(10)[[\"score\", \"job_titles\"]].copy()\n",
    "        df_q.insert(0, \"query\", query)\n",
    "        top10_blocks.append(df_q)\n",
    "        \n",
    "    # one combined CSV for all queries        \n",
    "    top10 = pd.concat(top10_blocks, ignore_index=True)\n",
    "    path = os.path.join(out_dir, f\"llm_top10__{model_tag}__all_queries.csv\")\n",
    "    top10.to_csv(path, index=True)\n",
    "    print(\"Saved:\", path)\n",
    "    \n",
    "    return top10, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 1928\n",
      "\n",
      "Query: data scientist\n",
      "    1.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    1.000  Aspiring Human Resources Professional\n",
      "    0.900  Aspiring Human Resources Specialist\n",
      "    0.890  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.740  Aspiring Human Resources Professional\n",
      "    0.720  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.700  HR Senior Specialist\n",
      "    0.690  Student at Chapman University\n",
      "Prompt tokens: 1928\n",
      "\n",
      "Query: machine learning engineer\n",
      "    1.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    1.000  Aspiring Human Resources Professional\n",
      "    0.900  Aspiring Human Resources Specialist\n",
      "    0.890  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.740  Aspiring Human Resources Professional\n",
      "    0.720  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.700  HR Senior Specialist\n",
      "    0.690  Student at Chapman University\n",
      "Prompt tokens: 1927\n",
      "\n",
      "Query: backend developer\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    0.750  Aspiring Human Resources Specialist\n",
      "    0.700  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.700  Aspiring Human Resources Professional\n",
      "    0.700  People Development Coordinator at Ryan\n",
      "    0.700  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.700  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.700  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  People Development Coordinator at Ryan\n",
      "Prompt tokens: 1927\n",
      "\n",
      "Query: product manager\n",
      "    1.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    1.000  Aspiring Human Resources Professional\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  Aspiring Human Resources Specialist\n",
      "    0.900  Aspiring Human Resources Professional\n",
      "    0.890  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.700  HR Senior Specialist\n",
      "    0.690  Student at Chapman University\n",
      "    0.410  Native English Teacher at EPIK (English Program in Korea)\n",
      "Saved: outputs\\llm\\llm_top10__phi3_mini_4k__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_all, out_path = run_query_full(QUERIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86751029",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c030c12",
   "metadata": {},
   "source": [
    "### Step 6 - Experience with another small LLM: **Gemma-2-2b-it from Google**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339a6d",
   "metadata": {},
   "source": [
    "Free some GPU allocated memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d273108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM allocated (MB): 8.12646484375\n",
      "VRAM reserved  (MB): 24.0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 1) Move model to CPU first (helps release GPU contexts cleanly)\n",
    "try:\n",
    "    mdl.to(\"cpu\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 2) Delete big objects\n",
    "for obj in [\"pipe\", \"mdl\", \"tok\"]:\n",
    "    if obj in globals():\n",
    "        del globals()[obj]\n",
    "\n",
    "# 3) Garbage collect + empty CUDA cache\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# 4) Sanity check\n",
    "if torch.cuda.is_available():\n",
    "    print(\"VRAM allocated (MB):\", torch.cuda.memory_allocated() / (1024**2))\n",
    "    print(\"VRAM reserved  (MB):\", torch.cuda.memory_reserved() / (1024**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32ee0d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5570a9c2074e55a684c466309b1b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e27d9d22c14f678b533d9889b666eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "HF_TOKEN = os.getenv(\"llm_gemma\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float16 if device.type==\"cuda\" else None,\n",
    "    token=HF_TOKEN\n",
    ").to(device).eval()\n",
    "\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=mdl, tokenizer=tok, device=0 if device.type==\"cuda\" else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27f0eb",
   "metadata": {},
   "source": [
    "Define a newer **prompt builder** function with a one-shot prompt and being more specific with the matching job titles. Also it drops the `system role` options that we used with Phi-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ccf6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_all_chat(query: str, titles: list[str]) -> str:\n",
    "    # Numbered candidates\n",
    "    lines = \"\\n\".join(f\"{i+1}) {t}\" for i, t in enumerate(titles))\n",
    "\n",
    "    # Rubric + strict output format\n",
    "    rubric = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Rate each candidate with an integer 0–100 using the FULL scale:\\n\"\n",
    "        \"  90–100 = exact/near-exact role match\\n\"\n",
    "        \"  70–89  = same discipline or very similar role\\n\"\n",
    "        \"  40–69  = related/adjacent\\n\"\n",
    "        \"  20–39  = mostly unrelated\\n\"\n",
    "        \"  0–19   = completely unrelated\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \" - Prefer same functional domain as the query.\\n\"\n",
    "        \" - If the query is technical (data/ML/backend), HR/People titles are mostly unrelated.\\n\"\n",
    "        \" - Titles with 'Student' or 'Aspiring' get lower scores unless they explicitly match the role.\\n\"\n",
    "        \"Ignore employer names, locations, programs.\\n\"\n",
    "        \"Return EXACTLY one integer per line, in the SAME ORDER as the candidates.\\n\"\n",
    "        \"No numbering, no words, no punctuation.\"\n",
    "    )\n",
    "    example = \"Example for 3 candidates:\\n82\\n41\\n7\"\n",
    "    user_text = f'Query: \"{query}\"\\n\\nCandidates:\\n{lines}\\n\\n{example}'\n",
    "\n",
    "    # Use system role only if the tokenizer’s chat template supports it\n",
    "    tmpl = (getattr(tok, \"chat_template\", \"\") or \"\")\n",
    "    supports_system = \"system\" in tmpl\n",
    "\n",
    "    if supports_system:\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": rubric},\n",
    "            {\"role\": \"user\",   \"content\": user_text},\n",
    "        ]\n",
    "    else:\n",
    "        # Gemma path: fold rubric into the user turn\n",
    "        msgs = [{\"role\": \"user\", \"content\": rubric + \"\\n\\n\" + user_text}]\n",
    "\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55f0ccfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "System role not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTemplateError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Tiny subset check (no changes elsewhere)\u001b[39;00m\n\u001b[32m      2\u001b[39m _subset = titles[:\u001b[32m5\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m _demo = \u001b[43mbuild_prompt_all_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata scientist\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrompt tokens:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tok(_demo)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m      5\u001b[39m df_sub, raw_sub = score_all_titles_once(\u001b[33m\"\u001b[39m\u001b[33mdata scientist\u001b[39m\u001b[33m\"\u001b[39m, _subset, max_new_tokens=\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mbuild_prompt_all_chat\u001b[39m\u001b[34m(query, titles)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Gemma path: fold rubric into the user turn\u001b[39;00m\n\u001b[32m     36\u001b[39m     msgs = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: rubric + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + user_text}]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1640\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1637\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1639\u001b[39m template_kwargs = {**\u001b[38;5;28mself\u001b[39m.special_tokens_map, **kwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1640\u001b[39m rendered_chat, generation_indices = \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1652\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:521\u001b[39m, in \u001b[36mrender_jinja_template\u001b[39m\u001b[34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[39m\n\u001b[32m    519\u001b[39m     all_generation_indices.append(generation_indices)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     rendered_chat = \u001b[43mcompiled_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[32m    529\u001b[39m     final_message = chat[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\jinja2\\environment.py:1295\u001b[39m, in \u001b[36mTemplate.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.environment.concat(\u001b[38;5;28mself\u001b[39m.root_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\jinja2\\environment.py:942\u001b[39m, in \u001b[36mEnvironment.handle_exception\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source=source)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<template>:1\u001b[39m, in \u001b[36mtop-level template code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\jinja2\\sandbox.py:401\u001b[39m, in \u001b[36mSandboxedEnvironment.call\u001b[39m\u001b[34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __self.is_safe_callable(__obj):\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SecurityError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__obj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is not safely callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:446\u001b[39m, in \u001b[36m_compile_jinja_template.<locals>.raise_exception\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_exception\u001b[39m(message):\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2.exceptions.TemplateError(message)\n",
      "\u001b[31mTemplateError\u001b[39m: System role not supported"
     ]
    }
   ],
   "source": [
    "# Tiny subset check (no changes elsewhere)\n",
    "_subset = titles[:5]\n",
    "_demo = build_prompt_all_chat(\"data scientist\", _subset)\n",
    "print(\"Prompt tokens:\", len(tok(_demo)[\"input_ids\"]))\n",
    "df_sub, raw_sub = score_all_titles_once(\"data scientist\", _subset, max_new_tokens=60)\n",
    "print(df_sub[[\"score\",\"job_titles\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ee904a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "System role not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTemplateError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m top10_all, out_path = \u001b[43mrun_query_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERIES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemma-2-2b-it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrun_query_full\u001b[39m\u001b[34m(queries, model_tag)\u001b[39m\n\u001b[32m     11\u001b[39m top10_blocks = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     df_rank, raw = \u001b[43mscore_all_titles_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     print_ranking(query, df_rank, top_k=\u001b[32m10\u001b[39m)\n\u001b[32m     17\u001b[39m     df_q = df_rank.head(\u001b[32m10\u001b[39m)[[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjob_titles\u001b[39m\u001b[33m\"\u001b[39m]].copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mscore_all_titles_once\u001b[39m\u001b[34m(query, titles, max_new_tokens, build_fn)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore_all_titles_once\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, titles: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], max_new_tokens: \u001b[38;5;28mint\u001b[39m = \u001b[32m300\u001b[39m, build_fn=build_prompt_all_chat):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     prompt = \u001b[43mbuild_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrompt tokens:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tok(prompt)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m      4\u001b[39m     inputs = tok(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(mdl.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mbuild_prompt_all_chat\u001b[39m\u001b[34m(query, titles)\u001b[39m\n\u001b[32m     17\u001b[39m user = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCandidates:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m msgs = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: rubric},\n\u001b[32m     19\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,   \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user}]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1640\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1637\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1639\u001b[39m template_kwargs = {**\u001b[38;5;28mself\u001b[39m.special_tokens_map, **kwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1640\u001b[39m rendered_chat, generation_indices = \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1652\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:521\u001b[39m, in \u001b[36mrender_jinja_template\u001b[39m\u001b[34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[39m\n\u001b[32m    519\u001b[39m     all_generation_indices.append(generation_indices)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     rendered_chat = \u001b[43mcompiled_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[32m    529\u001b[39m     final_message = chat[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\jinja2\\environment.py:1295\u001b[39m, in \u001b[36mTemplate.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.environment.concat(\u001b[38;5;28mself\u001b[39m.root_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\jinja2\\environment.py:942\u001b[39m, in \u001b[36mEnvironment.handle_exception\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source=source)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<template>:1\u001b[39m, in \u001b[36mtop-level template code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\jinja2\\sandbox.py:401\u001b[39m, in \u001b[36mSandboxedEnvironment.call\u001b[39m\u001b[34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __self.is_safe_callable(__obj):\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SecurityError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__obj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is not safely callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Devs\\pyEnv-1\\venvs\\Pot-Tals_LLM_env\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:446\u001b[39m, in \u001b[36m_compile_jinja_template.<locals>.raise_exception\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_exception\u001b[39m(message):\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2.exceptions.TemplateError(message)\n",
      "\u001b[31mTemplateError\u001b[39m: System role not supported"
     ]
    }
   ],
   "source": [
    "top10_all, out_path = run_query_full(QUERIES, \"gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pot_Tals_LLM_Env",
   "language": "python",
   "name": "pot-tals_llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
