{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## Potential Talents - Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "# Job Title Similarity using LLMs-as-Rankers\n",
    "\n",
    "### Objective\n",
    "Given a query, ask a small LLM to score **all 104 job titles at once** (0–100, one score per line, same order), then rank the scores to compare the **top-10** with other results (from embeddings + cosine or other LLMs results)\n",
    "\n",
    "### Constraints\n",
    "- Local GPU: **GTX 1080 Ti**.\n",
    "- **Deterministic** generation: `do_sample=False`, `num_beams=1`.\n",
    "\n",
    "### Models (initial)\n",
    "- **1:** `microsoft/phi-3-mini-4k-instruct` (4k context, small & GPU-friendly).\n",
    "- **2:** `google/gemma-2-2b-it` (8k context, very small).\n",
    "- **3:** `qwen2.5-3B-instruct` (32k context, ~3B params, list-style outputs).\n",
    "- (After some tests we will avoid FLAN-T5 here due to the ~512 token input limit.)\n",
    "\n",
    "### Method\n",
    "1) Load SBERT top-10 baseline (from Part 3).  \n",
    "2) Load a small **causal LM**.  \n",
    "3) Build a prompt that lists all **104** titles (numbered).  \n",
    "4) Generate **104 lines of integers**; parse → rank; print top-10; save top-10 CSV (`query,score,job_titles`).  \n",
    "5) Repeat for the 4 queries; later compute nDCG@10 and compare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "### Step 0 - Imports, config, folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ce1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core\n",
    "import os, json, math, re, random, time, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# HF\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2188f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "SEED = 23\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# paths\n",
    "DATA_DIR = \"data\"\n",
    "OUT_DIR  = \"outputs\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "QUERIES = [\"data scientist\", \"machine learning engineer\", \"backend developer\", \"product manager\"]  # same queries from Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d545eb",
   "metadata": {},
   "source": [
    "### Step 1 - Load titles and make a clean field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d845f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"potential_talents.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b98037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,\n",
       " ['2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional',\n",
       "  'Native English Teacher at EPIK (English Program in Korea)',\n",
       "  'Aspiring Human Resources Professional',\n",
       "  'People Development Coordinator at Ryan',\n",
       "  'Advisory Board Member at Celal Bayar University'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = df[\"job_title\"].astype(str).tolist()\n",
    "len(titles), titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c7ee5",
   "metadata": {},
   "source": [
    "### Step 2 - Load SBERT top-10 baseline (as-is, from the previous project part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb96aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            query     score                                         job_titles\n",
      "0  data scientist  0.595830  Information Systems Specialist and Programmer ...\n",
      "1  data scientist  0.494619                       Human Resources Professional\n",
      "2  data scientist  0.456588           Junior MES Engineer| Information Systems\n",
      "Queries in baseline: ['data scientist' 'machine learning engineer' 'backend developer'\n",
      " 'product manager']\n"
     ]
    }
   ],
   "source": [
    "# Load your SBERT baseline as produced in Part 3 (no changes to schema)\n",
    "BASELINE_TOP10_CSV = os.path.join(OUT_DIR, \"sbert_ranking_output.csv\")\n",
    "base = pd.read_csv(BASELINE_TOP10_CSV)\n",
    "\n",
    "print(base.head(3))\n",
    "print(\"Queries in baseline:\", base[\"query\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546e899",
   "metadata": {},
   "source": [
    "### Step 3 - Pretty printer (same style as Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2643cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ranking(query, rows_df, score_col=\"score\", title_col=\"job_titles\", top_k=10):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for _, r in rows_df.head(top_k).iterrows():\n",
    "        print(f\"   {r[score_col]: .3f}  {r[title_col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8800280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: data scientist\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n",
      "\n",
      "Query: backend developer\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n",
      "\n",
      "Query: product manager\n",
      "    0.596  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.495  Human Resources Professional\n",
      "    0.457  Junior MES Engineer| Information Systems\n",
      "    0.450  Aspiring Human Resources Specialist\n",
      "    0.449  Human Resources professional for the world leader in GIS software\n",
      "    0.441  HR Senior Specialist\n",
      "    0.433  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.416  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "    0.410  Student\n",
      "    0.403  Human Resources Specialist at Luxottica\n"
     ]
    }
   ],
   "source": [
    "for query in QUERIES:\n",
    "    print_ranking(query, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db80fed",
   "metadata": {},
   "source": [
    "### Step 4 - Load a small LLM (Phi-3-mini from Microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27da14b",
   "metadata": {},
   "source": [
    "**Phi-3 Mini (Microsoft)**\n",
    "- **Release Date**: April 23, 2024.\n",
    "- **Architecture**: Decoder-only (autoregressive).\n",
    "- **Parameters**: ~3.8B.\n",
    "- **Layers**: 32 transformer blocks, 32 attention heads.\n",
    "- **Context Window**: 4k tokens.\n",
    "- **Tokenizer**: SentencePiece-like (subword BPE).\n",
    "- **Objective**: Next-token prediction, trained as a general causal LM.\n",
    "- **Training**: Mixture of web, code, math, scientific texts; instruction-tuned for dialogue.\n",
    "- **Efficiency**: Optimized for small GPUs (runs on 8–12GB VRAM), strong FP16/INT8 support.\n",
    "- **License**: MIT-style permissive.\n",
    "- **Notes**: Very lightweight, deterministic, good for structured tasks on consumer GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5c1958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124\n",
      "built with CUDA: 12.4\n",
      "cuda available: True\n",
      "gpu: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"built with CUDA:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef92698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31501abf6d94e2aa0104515ac606fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "394583c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a proper chat prompt\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a calculator. Reply with digits only.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"Return the number 7.\"}\n",
    "]\n",
    "prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca4e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode & generate (greedy)\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "eos = [tok.eos_token_id]\n",
    "try:\n",
    "    eos.append(tok.convert_tokens_to_ids(\"<|end|>\"))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e8d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = mdl.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=3,\n",
    "    eos_token_id=eos,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e410f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "out = tok.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "print(out)  # -> 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f9861",
   "metadata": {},
   "source": [
    "### Step 5 — turn LLM into a job title ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8837f",
   "metadata": {},
   "source": [
    "We will turn the LLM into a ranker by asking it to assign an integer score (0–100) to each raw job title for a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07e1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_all_chat_phi(query: str, titles: list[str]) -> str:\n",
    "    lines = \"\\n\".join(f\"{i+1}) {t}\" for i, t in enumerate(titles))\n",
    "    rubric = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Rate each candidate with an integer 0–100 using the FULL scale:\\n\"\n",
    "        \" • 90–100 = exact/near-exact role match\\n\"\n",
    "        \" • 70–89  = same discipline or very similar role\\n\"\n",
    "        \" • 40–69  = related/adjacent\\n\"\n",
    "        \" • 10–39  = mostly unrelated\\n\"\n",
    "        \" • 0–9    = completely unrelated\\n\"\n",
    "        \"Use diverse scores; do NOT give 0 or 100 to many candidates.\\n\"\n",
    "        \"Ignore employer names, locations, programs.\\n\"\n",
    "        \"Output EXACTLY one integer per line, in the SAME ORDER as the candidates. No words, no punctuation.\"\n",
    "    )\n",
    "    # Non-extreme example\n",
    "    example = \"Example for 3 candidates:\\n82\\n41\\n7\"\n",
    "    user = f'Query: \"{query}\"\\n\\nCandidates:\\n{lines}\\n\\n{example}'\n",
    "    msgs = [{\"role\": \"system\", \"content\": rubric},\n",
    "            {\"role\": \"user\",   \"content\": user}]\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd5fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scores_n(out: str, n: int) -> list[int]:\n",
    "    # prefer last int per non-empty line; fallback to last N ints in whole text\n",
    "    lines = [l.strip() for l in out.splitlines() if l.strip()]\n",
    "    scores = []\n",
    "    \n",
    "    for line in lines:\n",
    "        ints = re.findall(r\"-?\\d+\", line)\n",
    "        if ints:\n",
    "            scores.append(int(ints[-1]))\n",
    "        if len(scores) >= n:\n",
    "            break\n",
    "        \n",
    "    if len(scores) < n:\n",
    "        all_ints = [int(x) for x in re.findall(r\"-?\\d+\", out)]\n",
    "        scores = all_ints[-n:]\n",
    "        \n",
    "    scores = [max(0, min(100, int(s))) for s in scores]\n",
    "    \n",
    "    # if still short, add a padding\n",
    "    if len(scores) < n:  \n",
    "        scores += [0] * (n - len(scores))\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b170f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_all_titles_once(query: str,\n",
    "                          titles: list[str],\n",
    "                          max_new_tokens: int = 300,\n",
    "                          build_fn=None):\n",
    "    build_fn = build_fn or build_prompt_all_chat_phi\n",
    "    prompt = build_fn(query, titles)\n",
    "    print(\"Prompt tokens:\", len(tok(prompt)[\"input_ids\"]))\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "    gen = mdl.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=[tok.eos_token_id],\n",
    "        min_new_tokens=min(len(titles), max_new_tokens-1),\n",
    "    )\n",
    "    out_text = tok.decode(gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    scores = parse_scores_n(out_text, len(titles))\n",
    "    df = pd.DataFrame({\"idx\": range(len(titles)), \"score\": scores})\n",
    "    df[\"job_titles\"] = [titles[i] for i in df[\"idx\"]]\n",
    "    df = df.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return df, out_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ee9d3",
   "metadata": {},
   "source": [
    "Test **build_prompt_all_chat**, **parse_scores_n** and **score_all_tittles_once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1c94d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEMO PROMPT (first 30 lines) ===\n",
      "<|system|>\n",
      "You are a recruiter scoring job-title similarity to the query.\n",
      "Rate each candidate with an integer 0–100 using the FULL scale:\n",
      " • 90–100 = exact/near-exact role match\n",
      " • 70–89  = same discipline or very similar role\n",
      " • 40–69  = related/adjacent\n",
      " • 10–39  = mostly unrelated\n",
      " • 0–9    = completely unrelated\n",
      "Use diverse scores; do NOT give 0 or 100 to many candidates.\n",
      "Ignore employer names, locations, programs.\n",
      "Output EXACTLY one integer per line, in the SAME ORDER as the candidates. No words, no punctuation.<|end|>\n",
      "<|user|>\n",
      "Query: \"data scientist\"\n",
      "\n",
      "Candidates:\n",
      "1) 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "2) Native English Teacher at EPIK (English Program in Korea)\n",
      "3) Aspiring Human Resources Professional\n",
      "4) People Development Coordinator at Ryan\n",
      "5) Advisory Board Member at Celal Bayar University\n",
      "\n",
      "Example for 3 candidates:\n",
      "82\n",
      "41\n",
      "7<|end|>\n",
      "<|assistant|>\n",
      "Token count (subset): 279\n"
     ]
    }
   ],
   "source": [
    "test_query = \"data scientist\"\n",
    "\n",
    "# A Tiny subset to inspect everything\n",
    "subset = titles[:5]\n",
    "\n",
    "demo_prompt = build_prompt_all_chat_phi(test_query, subset)\n",
    "print(\"=== DEMO PROMPT (first 30 lines) ===\")\n",
    "print(\"\\n\".join(demo_prompt.splitlines()[:30]))\n",
    "print(\"Token count (subset):\", len(tok(demo_prompt)[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0dd40f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 279\n",
      "\n",
      "=== RAW MODEL OUTPUT (subset) ===\n",
      "0\n",
      "0\n",
      "7\n",
      "41\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_sub, raw_sub = score_all_titles_once(test_query, subset, max_new_tokens=60)\n",
    "print(\"\\n=== RAW MODEL OUTPUT (subset) ===\")\n",
    "print(raw_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5738ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed scores (subset): [0, 0, 7, 41, 0]\n",
      "\n",
      "Paired (score, title) in ranked order:\n",
      " 41  People Development Coordinator at Ryan\n",
      "  7  Aspiring Human Resources Professional\n",
      "  0  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "  0  Native English Teacher at EPIK (English Program in Korea)\n",
      "  0  Advisory Board Member at Celal Bayar University\n"
     ]
    }
   ],
   "source": [
    "scores_sub = parse_scores_n(raw_sub, len(subset))\n",
    "print(\"\\nParsed scores (subset):\", scores_sub)\n",
    "print(\"\\nPaired (score, title) in ranked order:\")\n",
    "# the output DataFrame from `socre_all_titles_once`, df_sub, is sorted in not ascending order\n",
    "for _, r in df_sub.iterrows():\n",
    "    print(f\"{r['score']:>3}  {r['job_titles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bafef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token count (full): 1928\n"
     ]
    }
   ],
   "source": [
    "# B) One full run (preview only; avoids flooding output)\n",
    "full_prompt = build_prompt_all_chat_phi(test_query, titles)\n",
    "print(\"\\nToken count (full):\", len(tok(full_prompt)[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2197f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation of long strings\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d261d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 1928\n",
      "\n",
      "Full run: got 104 scores.\n",
      "Top-3 preview:\n",
      "   score                                                                                                job_titles\n",
      "0    100                                         Student at Humber College and Aspiring Human Resources Generalist\n",
      "1    100                                                           Advisory Board Member at Celal Bayar University\n",
      "2    100                                                                     Aspiring Human Resources Professional\n",
      "3     90                                                                       Aspiring Human Resources Specialist\n",
      "4     89                                         Student at Humber College and Aspiring Human Resources Generalist\n",
      "5     74                                                                     Aspiring Human Resources Professional\n",
      "6     72                                                 Native English Teacher at EPIK (English Program in Korea)\n",
      "7     70  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "8     70                                                                                      HR Senior Specialist\n",
      "9     69                                                                             Student at Chapman University\n"
     ]
    }
   ],
   "source": [
    "df_full, raw_full = score_all_titles_once(test_query, titles, max_new_tokens=300)\n",
    "print(\"\\nFull run: got\", len(df_full), \"scores.\")\n",
    "print(\"Top-3 preview:\")\n",
    "print(df_full.head(10)[[\"score\", \"job_titles\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ce5f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ranking(query, rows_df, top_k=10):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for _, r in rows_df.head(top_k).iterrows():\n",
    "        print(f\"   {r['score']/100: .3f}  {r['job_titles']}\")\n",
    "\n",
    "def run_query_full(queries: list[str],\n",
    "                   model_tag: str = \"phi3_mini_4k\",\n",
    "                   build_prompt_fn=None):\n",
    "    out_dir = os.path.join(OUT_DIR, \"llm\"); os.makedirs(out_dir, exist_ok=True)\n",
    "    top10_blocks = []\n",
    "    for query in queries:\n",
    "        df_rank, raw = score_all_titles_once(query, titles, build_fn=build_prompt_fn)\n",
    "        print_ranking(query, df_rank, top_k=10)\n",
    "        df_q = df_rank.head(10)[[\"score\", \"job_titles\"]].copy()\n",
    "        df_q.insert(0, \"query\", query)\n",
    "        top10_blocks.append(df_q)\n",
    "    top10 = pd.concat(top10_blocks, ignore_index=True)\n",
    "    path = os.path.join(out_dir, f\"llm_top10__{model_tag}__all_queries.csv\")\n",
    "    top10.to_csv(path, index=False)\n",
    "    print(\"Saved:\", path)\n",
    "    return top10, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 1928\n",
      "\n",
      "Query: data scientist\n",
      "    1.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    1.000  Aspiring Human Resources Professional\n",
      "    0.900  Aspiring Human Resources Specialist\n",
      "    0.890  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.740  Aspiring Human Resources Professional\n",
      "    0.720  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.700  HR Senior Specialist\n",
      "    0.690  Student at Chapman University\n",
      "Prompt tokens: 1928\n",
      "\n",
      "Query: machine learning engineer\n",
      "    1.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    1.000  Aspiring Human Resources Professional\n",
      "    0.900  Aspiring Human Resources Specialist\n",
      "    0.890  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.740  Aspiring Human Resources Professional\n",
      "    0.720  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.700  HR Senior Specialist\n",
      "    0.690  Student at Chapman University\n",
      "Prompt tokens: 1927\n",
      "\n",
      "Query: backend developer\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    0.750  Aspiring Human Resources Specialist\n",
      "    0.700  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.700  Aspiring Human Resources Professional\n",
      "    0.700  People Development Coordinator at Ryan\n",
      "    0.700  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.700  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.700  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.700  People Development Coordinator at Ryan\n",
      "Prompt tokens: 1927\n",
      "\n",
      "Query: product manager\n",
      "    1.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    1.000  Advisory Board Member at Celal Bayar University\n",
      "    1.000  Aspiring Human Resources Professional\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  Aspiring Human Resources Specialist\n",
      "    0.900  Aspiring Human Resources Professional\n",
      "    0.890  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.700  HR Senior Specialist\n",
      "    0.690  Student at Chapman University\n",
      "    0.410  Native English Teacher at EPIK (English Program in Korea)\n",
      "Saved: outputs\\llm\\llm_top10__phi3_mini_4k__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_phi, path_phi = run_query_full(\n",
    "    QUERIES,\n",
    "    model_tag=\"phi3_mini_4k__listwise\",\n",
    "    build_prompt_fn=build_prompt_all_chat_phi\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86751029",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c030c12",
   "metadata": {},
   "source": [
    "### Step 6 - Experience with another small LLM: **Gemma-2-2b-it from Google**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89602a3",
   "metadata": {},
   "source": [
    "**Gemma-2-2B-IT (Google)**\n",
    "- **Release Date**: June 27, 2024 (Gemma 2 family launch).\n",
    "- **Architecture**: Decoder-only (autoregressive).\n",
    "- **Parameters**: ~2.6B.\n",
    "- **Layers**: ~26 transformer layers (with RoPE).\n",
    "- **Context Window**: 8k tokens.\n",
    "- **Tokenizer**: SentencePiece (same family as PaLM-2 / Gemini).\n",
    "- **Objective**: Next-token prediction with instruction-tuning.\n",
    "- **Training**: Web-scale datasets filtered for quality, multilingual corpora.\n",
    "- **Efficiency**: Ultra-compact, designed for edge devices; runs well on 6–8GB GPUs.\n",
    "- **License**: Apache 2.0 (permissive).\n",
    "- *Notes*: Very small but instruction-tuned, produces stable integer list outputs if well-prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339a6d",
   "metadata": {},
   "source": [
    "Free some GPU allocated memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c681f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d273108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_GPU_memory():\n",
    "    def print_vram(prefix=\"\"):\n",
    "        if not torch.cuda.is_available():\n",
    "            print(prefix + \"CUDA not available\")\n",
    "            return\n",
    "        torch.cuda.synchronize()\n",
    "        alloc = torch.cuda.memory_allocated() / (1024**2)      # MiB\n",
    "        reserv = torch.cuda.memory_reserved() / (1024**2)      # MiB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / (1024**2)\n",
    "        print(f\"\\n{prefix}allocated: {alloc:.1f} MiB | reserved: {reserv:.1f} MiB | total: {total:,.0f} MiB\")\n",
    "\n",
    "    # Print memory allocation before freeing it\n",
    "    print(\"Measure memory usage before and after freeing it\")\n",
    "    print_vram(\"Before:\\n\")\n",
    "\n",
    "    # move model to CPU + delete big refs\n",
    "    try: mdl.to(\"cpu\")\n",
    "    except: pass\n",
    "    # free memory\n",
    "    for name in (\"pipe\",\"mdl\",\"tok\",\"inputs\",\"gen\"):\n",
    "        if name in globals(): del globals()[name]\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print_vram(\"After:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34982006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure memory usage before and after freeing it\n",
      "\n",
      "Before:\n",
      "allocated: 7296.5 MiB | reserved: 8596.0 MiB | total: 11,264 MiB\n",
      "\n",
      "After:\n",
      "allocated: 8.1 MiB | reserved: 20.0 MiB | total: 11,264 MiB\n"
     ]
    }
   ],
   "source": [
    "free_GPU_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32ee0d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899058e038fc4c93a3e979c2ffe19b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "HF_TOKEN = os.getenv(\"llm_gemma\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float16 if device.type==\"cuda\" else None,\n",
    "    token=HF_TOKEN\n",
    ").to(device).eval()\n",
    "\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=mdl, tokenizer=tok, device=0 if device.type==\"cuda\" else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27f0eb",
   "metadata": {},
   "source": [
    "Define a newer **prompt builder** function with a one-shot prompt and being more specific with the matching job titles. Also it drops the `system role` options that we used with Phi-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ccf6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_prompt_all_chat_gemma(query: str, titles: list[str]) -> str:\n",
    "    lines = \"\\n\".join(f\"{i+1}) {t}\" for i, t in enumerate(titles))\n",
    "    rubric = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Rate each candidate with an integer between zero and one hundred using the full scale.\\n\"\n",
    "        \"Use diverse scores; avoid giving many zeros or many hundreds.\\n\"\n",
    "        \"Ignore employer names, locations, and programs.\\n\"\n",
    "        \"Return EXACTLY one integer per line in the SAME ORDER as the candidates.\\n\"\n",
    "        \"No words, no punctuation, no numbering.\\n\"\n",
    "    )\n",
    "    user_text = f'Query: \"{query}\"\\n\\nCandidates:\\n{lines}\\n\\nSCORES:'\n",
    "    # Gemma’s template may not support a system role → fold rubric into the user turn\n",
    "    msgs = [{\"role\": \"user\", \"content\": rubric + \"\\n\\n\" + user_text}]\n",
    "    return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 1583\n",
      "\n",
      "Query: data scientist\n",
      "    0.200  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.100  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.100  Aspiring Human Resources Professional\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  Advisory Board Member at Celal Bayar University\n",
      "    0.100  Aspiring Human Resources Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  HR Senior Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  Seeking Human Resources HRIS and Generalist Positions\n",
      "Prompt tokens: 1584\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.200  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.100  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.100  Aspiring Human Resources Professional\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  Advisory Board Member at Celal Bayar University\n",
      "    0.100  Aspiring Human Resources Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  HR Senior Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  Seeking Human Resources HRIS and Generalist Positions\n",
      "Prompt tokens: 1583\n",
      "\n",
      "Query: backend developer\n",
      "    0.200  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.100  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.100  Aspiring Human Resources Professional\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  Advisory Board Member at Celal Bayar University\n",
      "    0.100  Aspiring Human Resources Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  HR Senior Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  Seeking Human Resources HRIS and Generalist Positions\n",
      "Prompt tokens: 1583\n",
      "\n",
      "Query: product manager\n",
      "    0.300  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.250  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.250  People Development Coordinator at Ryan\n",
      "    0.250  Aspiring Human Resources Specialist\n",
      "    0.200  Aspiring Human Resources Professional\n",
      "    0.200  Advisory Board Member at Celal Bayar University\n",
      "    0.200  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.200  HR Senior Specialist\n",
      "    0.200  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.200  Seeking Human Resources HRIS and Generalist Positions\n",
      "Saved: outputs\\llm\\llm_top10__gemma-2-2b-it__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_gemma, path_gemma = run_query_full(\n",
    "    QUERIES,\n",
    "    model_tag=\"gemma-2-2b-it__listwise\",\n",
    "    build_prompt_fn=build_prompt_all_chat_gemma\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf58ee",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2edcb",
   "metadata": {},
   "source": [
    "### Step 7 - Experience with another small LLM: **Qwen2.5-3B-Instruct from Qwen**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f3a4d",
   "metadata": {},
   "source": [
    "**Qwen2.5-3B-Instruct (Alibaba / Qwen Team)**\n",
    "- **Release Date**: September 5, 2024 (Qwen2.5 family release).\n",
    "- **Architecture**: Decoder-only (autoregressive).\n",
    "- **Parameters**: ~2.7–3B.\n",
    "- **Layers**: 28 transformer layers, 32 attention heads.\n",
    "- **Context Window**: 32k tokens (longest among your three).\n",
    "- **Tokenizer**: Custom BPE with multilingual coverage.\n",
    "- **Objective**: Next-token prediction, instruction-tuned with ChatML formatting.\n",
    "- **Training**: Massive multilingual web + code datasets, plus safety/alignment finetuning.\n",
    "- **Efficiency**: Larger context needs more VRAM, but still runnable on 12GB with FP16/INT8.\n",
    "- **License**: Apache 2.0.\n",
    "- *Notes*: Very strong for structured outputs (list-style, JSON); context length makes it robust for 104-title scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a40cf0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure memory usage before and after freeing it\n",
      "\n",
      "Before:\n",
      "allocated: 4995.6 MiB | reserved: 5710.0 MiB | total: 11,264 MiB\n",
      "\n",
      "After:\n",
      "allocated: 8.1 MiB | reserved: 20.0 MiB | total: 11,264 MiB\n"
     ]
    }
   ],
   "source": [
    "free_GPU_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "552fa1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8784a8ddf8f4776ab13f5a801bfd363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "079e660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen-specific prompt builder (ChatML-friendly, explicit N, hard stop)\n",
    "def build_prompt_all_chat_qwen(query: str, titles: list[str]) -> str:\n",
    "    n = len(titles)\n",
    "    lines = \"\\n\".join(f\"{i}) {t}\" for i, t in enumerate(titles, start=1))\n",
    "\n",
    "    rubric = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Score each candidate with an integer 0–100 using the FULL scale:\\n\"\n",
    "        \"  90–100 = exact/near-exact role match\\n\"\n",
    "        \"  70–89  = same discipline or very similar role\\n\"\n",
    "        \"  40–69  = related/adjacent\\n\"\n",
    "        \"  20–39  = mostly unrelated\\n\"\n",
    "        \"  0–19   = completely unrelated\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \" - Prefer same functional domain as the query.\\n\"\n",
    "        \" - If the query is technical (data/ML/backend), HR/People titles are mostly unrelated.\\n\"\n",
    "        \" - Titles with 'Student' or 'Aspiring' get lower scores unless they explicitly match the role.\\n\"\n",
    "        f\"Output EXACTLY {n} integers, one per line, in the SAME ORDER as the candidates.\\n\"\n",
    "        \"No words, no commas, no numbering, no punctuation.\\n\"\n",
    "        f\"After the {n}th line, output the token <END> and stop.\"\n",
    "    )\n",
    "\n",
    "    # very small one-shot to demonstrate format (3 lines + <END>)\n",
    "    # keep it generic so it transfers across queries\n",
    "    example = (\n",
    "        \"Example (3 candidates):\\n\"\n",
    "        \"Candidates:\\n\"\n",
    "        \"1) Senior Data Scientist\\n\"\n",
    "        \"2) HR Coordinator\\n\"\n",
    "        \"3) Retail Cashier\\n\"\n",
    "        \"Expected output:\\n\"\n",
    "        \"95\\n\"\n",
    "        \"20\\n\"\n",
    "        \"0\\n\"\n",
    "        \"<END>\"\n",
    "    )\n",
    "\n",
    "    user_text = (\n",
    "        f\"{rubric}\\n\\n\"\n",
    "        f'Query: \"{query}\"\\n\\n'\n",
    "        f\"Candidates:\\n{lines}\\n\\n\"\n",
    "        f\"{example}\"\n",
    "    )\n",
    "\n",
    "    # Qwen supports system; if anything fails, fall back to user-only.\n",
    "    msgs_sys = [\n",
    "        {\"role\": \"system\", \"content\": \"You are precise and output only the requested numbers.\"},\n",
    "        {\"role\": \"user\",   \"content\": user_text},\n",
    "    ]\n",
    "    msgs_user = [{\"role\": \"user\", \"content\": user_text}]  # fallback\n",
    "\n",
    "    try:\n",
    "        return tok.apply_chat_template(msgs_sys, tokenize=False, add_generation_prompt=True)\n",
    "    except Exception:\n",
    "        return tok.apply_chat_template(msgs_user, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 1774\n",
      "\n",
      "Query: data scientist\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.900  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.900  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  Student at Humber College and Aspiring Human Resources Generalist\n",
      "Prompt tokens: 1775\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.400  People Development Coordinator at Ryan\n",
      "    0.000  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.000  Aspiring Human Resources Professional\n",
      "    0.000  Advisory Board Member at Celal Bayar University\n",
      "    0.000  Aspiring Human Resources Specialist\n",
      "    0.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.000  HR Senior Specialist\n",
      "    0.000  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.000  Seeking Human Resources HRIS and Generalist Positions\n",
      "Prompt tokens: 1774\n",
      "\n",
      "Query: backend developer\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.900  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.900  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  Student at Humber College and Aspiring Human Resources Generalist\n",
      "Prompt tokens: 1774\n",
      "\n",
      "Query: product manager\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.900  Native English Teacher at EPIK (English Program in Korea)\n",
      "    0.900  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.900  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.900  People Development Coordinator at Ryan\n",
      "    0.900  Student at Humber College and Aspiring Human Resources Generalist\n",
      "Saved: outputs\\llm\\llm_top10__qwen2.5-3b-instruct__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_qwen, path_qwen = run_query_full(\n",
    "    QUERIES,\n",
    "    model_tag=\"qwen2.5-3b-instruct__listwise\",\n",
    "    build_prompt_fn=build_prompt_all_chat_qwen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f46ea",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b188ef",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fba51",
   "metadata": {},
   "source": [
    "As a reference, this is the **output using ChatGPT5**:\n",
    "\n",
    "Top matches (title • score/100)\n",
    "- Business Intelligence and Analytics at Travelers • 0.78\n",
    "- Information Systems Specialist and Programmer with a love for data and organization. • 0.62\n",
    "- Junior MES Engineer | Information Systems • 0.56\n",
    "- Undergraduate Research Assistant at Styczynski Lab • 0.54\n",
    "- Liberal Arts Major. Aspiring Human Resources Analyst. • 0.42\n",
    "- Seeking Human Resources HRIS and Generalist Positions • 0.28\n",
    "- Human Resources Generalist at Loparex • 0.25\n",
    "- Human Resources Specialist at Luxottica • 0.23\n",
    "- HR Senior Specialist • 0.22\n",
    "- Human Resources Professional • 0.20\n",
    "\n",
    "\n",
    "I used the following **prompt** (same used with Phi-3 mini): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc266b",
   "metadata": {},
   "source": [
    "You are a recruiter scoring job-title similarity to the query Rate each candidate with an integer 0–100 using the FULL scale: • 90–100 = exact/near-exact role match • 70–89 = same discipline or very similar role • 40–69 = related/adjacent • 10–39 = mostly unrelated • 0–9 = completely unrelated Use diverse scores; do NOT give 0 or 100 to many candidates. Ignore employer names, locations, programs. Output EXACTLY one integer per line, in the SAME ORDER as the candidates. No words, no punctuation. Example for return for 3 candidates: 82 41 7 --- Query: \"data scientist\" Candidates: \"2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional Native English Teacher at EPIK (English Program in Korea) Aspiring Human Resources Professional People Development Coordinator at Ryan Advisory Board Member at Celal Bayar University Aspiring Human Resources Specialist Student at Humber College and Aspiring Human Resources Generalist HR Senior Specialist Student at Humber College and Aspiring Human Resources Generalist Seeking Human Resources HRIS and Generalist Positions Student at Chapman University SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR Human Resources Coordinator at InterContinental Buckhead Atlanta 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional Native English Teacher at EPIK (English Program in Korea) Aspiring Human Resources Professional People Development Coordinator at Ryan 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional Native English Teacher at EPIK (English Program in Korea) Aspiring Human Resources Professional People Development Coordinator at Ryan Advisory Board Member at Celal Bayar University Aspiring Human Resources Specialist Student at Humber College and Aspiring Human Resources Generalist HR Senior Specialist Aspiring Human Resources Management student seeking an internship Seeking Human Resources Opportunities Aspiring Human Resources Management student seeking an internship Seeking Human Resources Opportunities 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional Native English Teacher at EPIK (English Program in Korea) Aspiring Human Resources Professional People Development Coordinator at Ryan Advisory Board Member at Celal Bayar University Aspiring Human Resources Specialist Student at Humber College and Aspiring Human Resources Generalist HR Senior Specialist Student at Humber College and Aspiring Human Resources Generalist Seeking Human Resources HRIS and Generalist Positions Student at Chapman University SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR Human Resources Coordinator at InterContinental Buckhead Atlanta 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional Native English Teacher at EPIK (English Program in Korea) Aspiring Human Resources Professional People Development Coordinator at Ryan Advisory Board Member at Celal Bayar University Aspiring Human Resources Specialist Student at Humber College and Aspiring Human Resources Generalist HR Senior Specialist Student at Humber College and Aspiring Human Resources Generalist Seeking Human Resources HRIS and Generalist Positions Student at Chapman University SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR Human Resources Coordinator at InterContinental Buckhead Atlanta 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional Aspiring Human Resources Professional People Development Coordinator at Ryan Aspiring Human Resources Specialist HR Senior Specialist Seeking Human Resources HRIS and Generalist Positions Student at Chapman University SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR Human Resources Coordinator at InterContinental Buckhead Atlanta Experienced Retail Manager and aspiring Human Resources Professional Human Resources, Staffing and Recruiting Professional Human Resources Specialist at Luxottica Director of Human Resources North America, Groupe Beneteau Retired Army National Guard Recruiter, office manager, seeking a position in Human Resources. Human Resources Generalist at ScottMadden, Inc. Business Management Major and Aspiring Human Resources Manager Aspiring Human Resources Manager, seeking internship in Human Resources. Human Resources Professional Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!! (408) 709-2621 Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment \"Human Resources| Conflict Management| Policies & Procedures|Talent Management|Benefits & Compensation\" Human Resources Generalist at Schwan's Liberal Arts Major. Aspiring Human Resources Analyst. Junior MES Engineer| Information Systems Senior Human Resources Business Partner at Heil Environmental Aspiring Human Resources Professional | An energetic and Team-Focused Leader HR Manager at Endemol Shine North America Human Resources professional for the world leader in GIS software RRP Brand Portfolio Executive at JTI (Japan Tobacco International) Information Systems Specialist and Programmer with a love for data and organization. Bachelor of Science in Biology from Victoria University of Wellington Human Resources Management Major Director Human Resources at EY Undergraduate Research Assistant at Styczynski Lab Lead Official at Western Illinois University Seeking employment opportunities within Customer Service or Patient Care Admissions Representative at Community medical center long beach Seeking Human Resources Opportunities. Open to travel and relocation. Student at Westfield State University \"Student at Indiana University Kokomo - Business Management - Retail Manager at Delphi Hardware and Paint\" Aspiring Human Resources Professional Student Seeking Human Resources Position Aspiring Human Resources Manager | Graduating May 2020 | Seeking an Entry-Level Human Resources Position in St. Louis Human Resources Generalist at Loparex Business Intelligence and Analytics at Travelers Always set them up for Success Director Of Administration at Excellence Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f7cb9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb4251",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45de73d",
   "metadata": {},
   "source": [
    "### Step 8 - Experience with Kimi K2 / Moonshot AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ce568",
   "metadata": {},
   "source": [
    "**Kimi K2 (K2-0711)/ Moonshot AI**\n",
    "- **Release Date**: July 11, 2025.\n",
    "- **Architecture**: Mixture-of-Experts (MoE) Transformer with MLA (Multi-head Latent Attention)\n",
    "- **Parameters**: ~1T total, ~32B activated (MoE).\n",
    "- **Layers**: 61 total (including 1 dense layer); 64 attention heads; 384 experts; 8 selected experts per token; 1 shared expert.\n",
    "- **Context Window**: 256K tokens (longest among your three).\n",
    "- **Tokenizer**: Custom tokenizer; covab size 160K.\n",
    "- **Objective**: Language Modeling (causal/next-token) with multi-stage post-training focused on agentic capabilities (tool use, planning) using RL variants.\n",
    "- **Training**: Pre-trained on 15.T tokens with the MuonChip optimizer; post-training includes large-scale agentic data synthesis and RLVR + self-critique.\n",
    "- **Efficiency**: MoE with 32B active params; block-FP8 checkpointsl recommended engines include vLLM, SGLang, KTransformers, TensorRT-LLM\n",
    "- **License**: Modified MIT (code and weights)\n",
    "- *Notes*: OpenAI/Anthropic-compatible API available via platform.moonshot.ai; recommended temperature ~ 0.6 for Instruct variants; strong tool-calling support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbab0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure memory usage before and after freeing it\n",
      "\n",
      "Before:\n",
      "allocated: 6002.6 MiB | reserved: 6798.0 MiB | total: 11,264 MiB\n",
      "\n",
      "After:\n",
      "allocated: 8.1 MiB | reserved: 20.0 MiB | total: 11,264 MiB\n"
     ]
    }
   ],
   "source": [
    "free_GPU_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1de0a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709f5cc",
   "metadata": {},
   "source": [
    "For this model we will use the MOONSHOT API. Therefor, we will read the API KEY from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ca16f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"MOONSHOT_API_KEY\")\n",
    "base_url = os.getenv(\"MOONSHOT_API_BASE\", \"https://api.moonshot.ai/v1\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"MOONSHOT_API_KEY is not set. Check your .env or environment.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90bfbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model (check Moonshot platform for latest stable/previews)\n",
    "KIMI_MODEL_ID = os.getenv(\"KIMI_MODEL_ID\", \"kimi-k2-0905-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f565ce4",
   "metadata": {},
   "source": [
    "##### Here we add a thin **Kimi (Moonshot)** adapter so the evaluation pipeline stays identical to previous local HF models: same rubric/prompt format, same strict parser (`parse_scores_n` → exactly **N** integers 0–100), same **ranking/CSV artifacts**, and **temperature=0** for determinism. \n",
    "\n",
    "##### The only change is the generation call (OpenAI-compatible Chat Completions API) wrapped by `score_all_titles_once_kimi` and `run_query_full_kimi`. This preserves apples-to-apples comparisons against Reference-A (ChatGPT-5) while letting swap models with just the prompt builder and the scorer entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79dd7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kimi prompt builder (simple, parser-friendly, same rubric)\n",
    "def build_scoring_prompt_kimi(query: str, candidates: list[str]) -> str:\n",
    "    n = len(candidates)\n",
    "    header = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query\\n\"\n",
    "        \"Rate each candidate with an integer 0–100 using the FULL scale:\\n\"\n",
    "        \"• 90–100 = exact/near-exact role match\\n\"\n",
    "        \"• 70–89  = same discipline or very similar role\\n\"\n",
    "        \"• 40–69  = related/adjacent\\n\"\n",
    "        \"• 10–39  = mostly unrelated\\n\"\n",
    "        \"• 0–9    = completely unrelated\\n\"\n",
    "        \"Use diverse scores; do NOT give 0 or 100 to many candidates.\\n\"\n",
    "        \"Ignore employer names, locations, programs.\\n\"\n",
    "        f\"Output EXACTLY {n} integers, one per line, in the SAME ORDER as the candidates.\\n\"\n",
    "        \"Integers ONLY (no decimals, no percentages, no words, no punctuation).\\n\\n\"\n",
    "        \"Example for 3 candidates:\\n82\\n41\\n7\\n\"\n",
    "        \"---\\n\"\n",
    "    )\n",
    "    return header + f'Query: \"{query}\"\\nCandidates:\\n' + \"\\n\".join(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f17ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kimi scorer (adapter with same return shape as your local scorer)\n",
    "def score_all_titles_once_kimi(query: str,\n",
    "                               titles: list[str],\n",
    "                               max_new_tokens: int = 1200,\n",
    "                               build_fn=build_scoring_prompt_kimi):\n",
    "    prompt = build_fn(query, titles)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=KIMI_MODEL_ID,         \n",
    "        temperature=0.1,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.2,\n",
    "        max_tokens=max_new_tokens,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    # debug code\n",
    "    # raw = resp.choices[0].message.content or \"\"\n",
    "    # lines = [ln for ln in raw.splitlines() if ln.strip()]\n",
    "    # print(\"parsed_ints:\", sum(ln.strip().isdigit() for ln in lines))\n",
    "    # print(\"unique_ints:\", len({int(ln.strip()) for ln in lines if ln.strip().isdigit()}))\n",
    "\n",
    "    out_text = resp.choices[0].message.content or \"\"\n",
    "    scores = parse_scores_n(out_text, len(titles))   # reuse the original parser\n",
    "    df = pd.DataFrame({\"idx\": range(len(titles)), \"score\": scores})\n",
    "    df[\"job_titles\"] = [titles[i] for i in df[\"idx\"]]\n",
    "    df = df.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return df, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e142dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper runner mirroring the run_query_full original function\n",
    "def run_query_full_kimi(queries: list[str],\n",
    "                        model_tag: str = \"kimi-k2-0905\",\n",
    "                        build_prompt_fn=build_scoring_prompt_kimi):\n",
    "    out_dir = os.path.join(OUT_DIR, \"llm\"); os.makedirs(out_dir, exist_ok=True)\n",
    "    top10_blocks = []\n",
    "    for query in queries:\n",
    "        df_rank, raw = score_all_titles_once_kimi(query, titles, build_fn=build_prompt_fn)\n",
    "        print_ranking(query, df_rank, top_k=10)   # <- reuse pretty-printer\n",
    "        df_q = df_rank.head(10)[[\"score\", \"job_titles\"]].copy()\n",
    "        df_q.insert(0, \"query\", query)\n",
    "        top10_blocks.append(df_q)\n",
    "    top10 = pd.concat(top10_blocks, ignore_index=True)\n",
    "    path = os.path.join(out_dir, f\"llm_top10__{model_tag}__all_queries.csv\")\n",
    "    top10.to_csv(path, index=False)\n",
    "    print(\"Saved:\", path)\n",
    "    return top10, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ff3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: data scientist\n",
      "    0.060  Human Resources Management Major\n",
      "    0.060  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.050  Junior MES Engineer| Information Systems\n",
      "    0.050  Director Human Resources  at EY\n",
      "    0.050  Retired Army National Guard Recruiter, office manager,  seeking a position in Human Resources.\n",
      "    0.050  Business Management Major and Aspiring Human Resources Manager\n",
      "    0.050  Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment\n",
      "    0.050  Human Resources Professional\n",
      "    0.050  RRP Brand Portfolio Executive at JTI (Japan Tobacco International)\n",
      "    0.050  Liberal Arts Major. Aspiring Human Resources Analyst.\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.060  Human Resources Specialist at Luxottica\n",
      "    0.050  Aspiring Human Resources Manager, seeking internship in Human Resources.\n",
      "    0.050  Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621\n",
      "    0.050  Human Resources Professional\n",
      "    0.050  Human Resources, Staffing and Recruiting Professional\n",
      "    0.050  Retired Army National Guard Recruiter, office manager,  seeking a position in Human Resources.\n",
      "    0.050  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.050  Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment\n",
      "    0.040  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.040  Business Management Major and Aspiring Human Resources Manager\n",
      "\n",
      "Query: backend developer\n",
      "    0.070  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.060  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.060  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.060  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.060  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.060  HR Senior Specialist\n",
      "    0.060  Human Resources Specialist at Luxottica\n",
      "    0.050  HR Senior Specialist\n",
      "    0.050  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.050  Native English Teacher at EPIK (English Program in Korea)\n",
      "\n",
      "Query: product manager\n",
      "    0.400  Director of Human Resources North America, Groupe Beneteau\n",
      "    0.390  Human Resources Specialist at Luxottica\n",
      "    0.380  Experienced Retail Manager and aspiring Human Resources Professional\n",
      "    0.370  Human Resources, Staffing and Recruiting Professional\n",
      "    0.360  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.350  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.340  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.330  HR Senior Specialist\n",
      "    0.320  Student at Chapman University\n",
      "    0.310  Aspiring Human Resources Professional\n",
      "Saved: outputs\\llm\\llm_top10__kimi-k2-0905-preview__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_kimi, path_kimi = run_query_full_kimi(\n",
    "    QUERIES,\n",
    "    model_tag=\"kimi-k2-0905-preview__listwise\",\n",
    "    build_prompt_fn=build_scoring_prompt_kimi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4905b",
   "metadata": {},
   "source": [
    "### Step 9 - Experience with LLaMA / Meta AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702356e",
   "metadata": {},
   "source": [
    "**LLaMA 3.2 3B Instruct / Meta AI**\n",
    "- **Release Date**: September 25, 2024.\n",
    "- **Architecture**: Autoregressive (decoder-only) Transformer with Grouped-Query Attention (GQA) and RoPE  .\n",
    "- **Parameters**: ~3.21B\n",
    "- **Layers**: 28 transformer layers, 24 attention heads, 8 KV heads, hidden size 3072 (per model configs) \n",
    "- **Context Window**: 128K tokens\n",
    "- **Tokenizer**: Llama-3 tokenizer (BPE) with 128,256 vocab size (vs. 32K in Llama 2).\n",
    "- **Objective**: Causal next-token LM; instruction models aligned for dialogue/agentic tasks.\n",
    "- **Training**: Pretrained on up to ~9T+ tokens; for 1B/3B, destillation from Llama 3.1 8B/70B was used; knowledge cutoff Dec 2023.\n",
    "- **Efficiency**: Designed for edge/on-device and small-GPU use; keeps the long 128 K context for large candidate lists.\n",
    "- **License**: Llama 3.2 Community License.\n",
    "- *Notes*: Use recent Transformers with the official Llama 3.x chat template for best adherence to constrained outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure memory usage before and after freeing it\n",
      "\n",
      "Before:\n",
      "allocated: 8.1 MiB | reserved: 20.0 MiB | total: 11,264 MiB\n",
      "\n",
      "After:\n",
      "allocated: 8.1 MiB | reserved: 20.0 MiB | total: 11,264 MiB\n"
     ]
    }
   ],
   "source": [
    "free_GPU_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0efe74",
   "metadata": {},
   "source": [
    "#### **\\\\!/** Note on running LLaMA locally (Windows, VRAM, and speed)\n",
    "\n",
    "I first tried `Llama 3.1 8B Instruct` in FP16/BF16 on Windows. With my GPU (~11 GB VRAM), the model didn’t fit, so device_map=\"auto\" offloaded layers to CPU.\n",
    "Result: **generation became very slow** (one run sat for ~28 minutes) because most of the compute happened on the CPU instead of the GPU.\n",
    "\n",
    "I also considered 4-bit quantization to fit 8B, but on Windows the **`bitsandbytes`** wheel isn’t reliably available/officially supported. It’s easier on Linux, but I’m avoiding that detour for now.\n",
    "\n",
    "Decision: switch to **`LLaMA 3 (3.2) 3B Instruct`**, which fits comfortably in ~11 GB and runs on the GPU at normal speed (no offload)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "246ee62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1ecb5a1b9149c3918e768a64cba214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c6d78fd970473ba5779420d30ee408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model: 3B Instruct (works well on ~11 GB VRAM)\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=os.getenv(\"HF_TOKEN\"), use_fast=True)\n",
    "\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else None,  # or torch.float16\n",
    "    device_map=\"auto\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    #attn_implementation=\"flash_attention_2\",  # let Transformers pick SDPA/eager automatically\n",
    ").eval()\n",
    "\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72271351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA prompt builder (same simple rubric + 3-line example)\n",
    "def build_prompt_all_chat_llama(query: str, titles: list[str]) -> str:\n",
    "    n = len(titles)\n",
    "    rubric = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Rate each candidate with an integer 0–100 using the FULL scale:\\n\"\n",
    "        \"• 90–100 = exact/near-exact • 70–89 = very similar • 40–69 = related\\n\"\n",
    "        \"• 10–39 = mostly unrelated • 0–9 = unrelated\\n\"\n",
    "        f\"Output EXACTLY {n} integers, one per line, in the SAME ORDER as the candidates.\\n\"\n",
    "        \"Integers ONLY (no decimals, no words, no punctuation). Use the full range; avoid identical scores.\\n\\n\"\n",
    "        f'Query: \"{query}\"\\n'\n",
    "        \"Candidates:\\n\" + \"\\n\".join(titles)\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are precise and output only the requested integers.\"},\n",
    "        {\"role\": \"user\", \"content\": rubric},\n",
    "    ]\n",
    "    try:\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except Exception:\n",
    "        return rubric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc42160",
   "metadata": {},
   "source": [
    "#### The rationale for introducing the new pairwise scoring functions:\n",
    "\n",
    "With small local models (e.g., LLaMA-3.2-3B on ~11 GB VRAM), the original *listwise prompt*,“scoring all 104 titles at once”, is brittle. The model must hold a long rubric plus 100+ candidates and emit exactly 104 integers in order. In practice this led to early stopping, example-echoing, flat/identical scores across queries, and heavy parser padding with zeros.\n",
    "\n",
    "**The new design scores one title at a time**: one compact prompt per (query, title) returning a single integer (0–100). This drastically lowers the cognitive and formatting burden, so the 3B model gives stable, query-dependent scores with greedy decoding. A simple one-int parser replaces the fragile N-line parser, and the final ranking comes from sorting the 104 individual scores.\n",
    "\n",
    "To **balance quality vs. cost**, we’ll use a hybrid approach:\n",
    "• For local small LMs → pairwise (robust, accurate on the available hardware).\n",
    "• For hosted/strong LMs (DeepSeek R1, ChatGPT-class, Kimi K2, etc.) → **listwise** (one shot per query is cheaper and fast, and these models follow the rubric well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90d6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Single-candidate prompt (zero-shot, integers only)\n",
    "def build_prompt_single_llama(query: str, title: str) -> str:\n",
    "    n = 1\n",
    "    instr = (\n",
    "        \"You are a recruiter scoring job-title similarity to the query.\\n\"\n",
    "        \"Return EXACTLY one integer 0–100.\\n\"\n",
    "        \"Scale:\\n\"\n",
    "        \" • 90–100 = exact/near-exact\\n\"\n",
    "        \" • 70–89  = very similar\\n\"\n",
    "        \" • 40–69  = related/adjacent\\n\"\n",
    "        \" • 10–39  = mostly unrelated\\n\"\n",
    "        \" • 0–9    = unrelated\\n\"\n",
    "        \"Return the integer ONLY (no words, no punctuation, no decimals).\"\n",
    "    )\n",
    "    user = f'Query: \"{query}\"\\nCandidate:\\n{title}'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instr},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "    ]\n",
    "    try:\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except Exception:\n",
    "        return instr + \"\\n\\n\" + user\n",
    "\n",
    "# 2) Parse exactly one integer\n",
    "def parse_one_int(text: str) -> int:\n",
    "    import re\n",
    "    m = re.findall(r\"-?\\d+\", text)\n",
    "    if not m: \n",
    "        return 0\n",
    "    x = int(m[-1])\n",
    "    return max(0, min(100, x))\n",
    "\n",
    "# 3) Pairwise scorer that preserves your downstream format\n",
    "def score_titles_llama_pairwise(query: str, all_titles: list[str], max_new_tokens: int = 8):\n",
    "    rows = []\n",
    "    for i, title in enumerate(all_titles):\n",
    "        prompt = build_prompt_single_llama(query, title)\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "        out_ids = mdl.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        out_txt = tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        score = parse_one_int(out_txt)\n",
    "        rows.append({\"idx\": i, \"score\": score, \"job_titles\": title})\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Runner that matches your existing CSV artifact shape\n",
    "def run_query_full_llama_pairwise(queries: list[str], model_tag: str = \"llama-3.2-3b-instruct-pairwise\"):\n",
    "    out_dir = os.path.join(OUT_DIR, \"llm\"); os.makedirs(out_dir, exist_ok=True)\n",
    "    top10_blocks = []\n",
    "    for q in queries:\n",
    "        df_rank = score_titles_llama_pairwise(q, titles)\n",
    "        print_ranking(q, df_rank, top_k=10)\n",
    "        df_q = df_rank.head(10)[[\"score\", \"job_titles\"]].copy()\n",
    "        df_q.insert(0, \"query\", q)\n",
    "        top10_blocks.append(df_q)\n",
    "    top10 = pd.concat(top10_blocks, ignore_index=True)\n",
    "    path = os.path.join(out_dir, f\"llm_top10__{model_tag}__all_queries.csv\")\n",
    "    top10.to_csv(path, index=False)\n",
    "    print(\"Saved:\", path)\n",
    "    return top10, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b826a9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: data scientist\n",
      "    0.700  Business Intelligence and Analytics at Travelers\n",
      "    0.390  Junior MES Engineer| Information Systems\n",
      "    0.300  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.300  Human Resources|\n",
      "Conflict Management|\n",
      "Policies & Procedures|Talent Management|Benefits & Compensation\n",
      "    0.100  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.100  Aspiring Human Resources Professional\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  Aspiring Human Resources Specialist\n",
      "    0.100  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.400  Junior MES Engineer| Information Systems\n",
      "    0.390  Business Intelligence and Analytics at Travelers\n",
      "    0.300  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.100  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.100  Student at Chapman University\n",
      "    0.100  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.100  Student at Chapman University\n",
      "    0.100  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.100  Student at Chapman University\n",
      "    0.100  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "\n",
      "Query: backend developer\n",
      "    0.400  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.400  Business Intelligence and Analytics at Travelers\n",
      "    0.390  Junior MES Engineer| Information Systems\n",
      "    0.300  Human Resources|\n",
      "Conflict Management|\n",
      "Policies & Procedures|Talent Management|Benefits & Compensation\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  Aspiring Human Resources Specialist\n",
      "    0.100  HR Senior Specialist\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  People Development Coordinator at Ryan\n",
      "    0.100  Aspiring Human Resources Specialist\n",
      "\n",
      "Query: product manager\n",
      "    0.700  Business Intelligence and Analytics at Travelers\n",
      "    0.600  Human Resources|\n",
      "Conflict Management|\n",
      "Policies & Procedures|Talent Management|Benefits & Compensation\n",
      "    0.600  RRP Brand Portfolio Executive at JTI (Japan Tobacco International)\n",
      "    0.400  Student at Indiana University Kokomo - Business Management - \n",
      "Retail Manager at Delphi Hardware and Paint\n",
      "    0.390  Information Systems Specialist and Programmer with a love for data and organization.\n",
      "    0.390  Director Of Administration at Excellence Logging\n",
      "    0.390  Aspiring Human Resources Professional | An energetic and Team-Focused Leader\n",
      "    0.390  Experienced Retail Manager and aspiring Human Resources Professional\n",
      "    0.390  Junior MES Engineer| Information Systems\n",
      "    0.300  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "Saved: outputs\\llm\\llm_top10__llama-3.2-3b-instruct-pairwise__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_llama, path_llama = run_query_full_llama_pairwise(\n",
    "    QUERIES,\n",
    "    model_tag=\"llama-3.2-3b-instruct-pairwise\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508f695",
   "metadata": {},
   "source": [
    "### Step 10 - Experience with DeepSeek R1 / DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dab05",
   "metadata": {},
   "source": [
    "**LLaMA 3.1 / Meta AI**\n",
    "- **Release Date**: July 23, 2024.\n",
    "- **Architecture**: Autoregressive (decoder-only) Transformer with Grouped-Query Attention (GQA).\n",
    "- **Parameters**: ~8B, ~70B and ~405B variants.\n",
    "- **Layers**: \n",
    "        - 8B: 32 transformer layers, 32 attention heads, 8 KV heads, hidden size 4096, intermediate size 14336.\n",
    "        - 70B: 80 transformer layers, 64 attention heads.\n",
    "- **Context Window**: 128K tokens (all 3.1 sizes).\n",
    "- **Tokenizer**: New Llama 3 tokenizer (SentencePiece/BPE) with 128,256 vocab size (vs. 32K in Llama 2).\n",
    "- **Objective**: Next-token prediction; instruction models aligned with superviced fine-tuning (SFT) and RLHF.\n",
    "- **Training**: Pretrained on ~15T+ tokens; knowledge cutoff December 2023; multilingual coverage (8 supported languages)\n",
    "- **Efficiency**: GQA for scalable inference; strong ecosystem support in Transformers (FlashAttention 2, 4-bit, quitization via bitsandbytes)\n",
    "- **License**: Llama 3.1 Community License.\n",
    "- *Notes*: Official chat template / tool-use formats provided; use Transformers >= 4.43 with Llama 3.1 prompt format for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef0bb035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure memory usage before and after freeing it\n",
      "\n",
      "Before:\n",
      "allocated: 6136.0 MiB | reserved: 7314.0 MiB | total: 11,264 MiB\n",
      "\n",
      "After:\n",
      "allocated: 8.1 MiB | reserved: 20.0 MiB | total: 11,264 MiB\n"
     ]
    }
   ],
   "source": [
    "free_GPU_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2299ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env + build client\n",
    "load_dotenv()\n",
    "DS_API_KEY  = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "DS_API_BASE = os.getenv(\"DEEPSEEK_API_BASE\")\n",
    "\n",
    "if not DS_API_KEY:\n",
    "    raise RuntimeError(\"DEEPSEEK_API_KEY is not set. Add it to your .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2db2066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ds = OpenAI(api_key=DS_API_KEY, base_url=DS_API_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5d3bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "DEEPSEEK_MODEL_ID = \"deepseek-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e4bba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: data scientist\n",
      "    0.060  Aspiring Human Resources Professional | An energetic and Team-Focused Leader\n",
      "    0.060  Senior Human Resources Business Partner at Heil Environmental\n",
      "    0.050  RRP Brand Portfolio Executive at JTI (Japan Tobacco International)\n",
      "    0.050  HR Manager at Endemol Shine North America\n",
      "    0.050  Director of Human Resources North America, Groupe Beneteau\n",
      "    0.050  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.050  Lead Official at Western Illinois University\n",
      "    0.050  Human Resources Generalist at Schwan's\n",
      "    0.050  Human Resources Management Major\n",
      "    0.050  Undergraduate Research Assistant at Styczynski Lab\n",
      "\n",
      "Query: machine learning engineer\n",
      "    0.150  Human Resources|\n",
      "Conflict Management|\n",
      "Policies & Procedures|Talent Management|Benefits & Compensation\n",
      "    0.080  Student at Chapman University\n",
      "    0.080  Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621\n",
      "    0.070  Human Resources, Staffing and Recruiting Professional\n",
      "    0.070  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.070  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.070  Human Resources Generalist at Schwan's\n",
      "    0.070  Human Resources Generalist at ScottMadden, Inc.\n",
      "    0.070  Aspiring Human Resources Professional | Passionate about helping to create an inclusive and engaging work environment\n",
      "    0.060  Aspiring Human Resources Specialist\n",
      "\n",
      "Query: backend developer\n",
      "    0.060  Student at Chapman University\n",
      "    0.050  Seeking Human Resources Opportunities\n",
      "    0.050  2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "    0.050  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.050  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.040  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.040  HR Senior Specialist\n",
      "    0.040  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.040  Student at Humber College and Aspiring Human Resources Generalist\n",
      "    0.040  HR Senior Specialist\n",
      "\n",
      "Query: product manager\n",
      "    0.300  Director of Human Resources North America, Groupe Beneteau\n",
      "    0.290  Human Resources Specialist at Luxottica\n",
      "    0.280  Human Resources, Staffing and Recruiting Professional\n",
      "    0.270  Experienced Retail Manager and aspiring Human Resources Professional\n",
      "    0.260  Human Resources Coordinator at InterContinental Buckhead Atlanta\n",
      "    0.250  SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR\n",
      "    0.240  Student at Chapman University\n",
      "    0.230  Seeking Human Resources HRIS and Generalist Positions\n",
      "    0.220  HR Senior Specialist\n",
      "    0.210  Aspiring Human Resources Specialist\n",
      "Saved: outputs\\llm\\llm_top10__deepseek-chat__listwise__all_queries.csv\n"
     ]
    }
   ],
   "source": [
    "top10_DS, path_DS = run_query_full_kimi(\n",
    "    QUERIES,\n",
    "    model_tag=\"deepseek-chat__listwise\",\n",
    "    build_prompt_fn=build_scoring_prompt_kimi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48463f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pot_Tals_LLM_Env",
   "language": "python",
   "name": "pot-tals_llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
